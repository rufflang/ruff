# Ruff Field Notes — Phase 2 VM Bytecode Optimizations

**Date:** 2026-01-27
**Session:** 07:46 PST
**Branch/Commit:** main / 5d150dd
**Scope:** Implemented Phase 2 VM bytecode optimizations including constant folding, dead code elimination, and peephole optimizations. Created new optimizer module with 3 optimization passes. All 198 tests pass with zero regressions.

---

## What I Changed

- **Created `src/optimizer.rs`** (650+ lines)
  - Three optimization passes: constant folding, dead code elimination, peephole optimization
  - Recursive optimization for nested functions in constant pool
  - Comprehensive statistics tracking (OptimizationStats struct)
  - 6 unit tests for optimizer correctness

- **Modified `src/compiler.rs`**
  - Added `compile_with_optimization()` method
  - Integrated optimizer into compilation pipeline
  - Optimization runs automatically after bytecode generation
  - Debug logging for optimization statistics

- **Updated module declarations**
  - Added `mod optimizer;` to `src/main.rs` and `src/lib.rs`
  - Proper visibility for optimizer public API

- **Created `tests/test_vm_optimizations.ruff`**
  - 15 test scenarios covering all optimization types
  - Demonstrates constant folding (19 constants), dead code elimination (44 instructions), peephole opts (2)
  - Tests runtime correctness after optimizations

- **Updated documentation**
  - `CHANGELOG.md` - Detailed Phase 2 optimization features
  - `ROADMAP.md` - Marked Phase 2 complete, highlighted Phase 3 (JIT) as next priority
  - `README.md` - Updated status with optimization results

- **Git commits** (following AGENT_INSTRUCTIONS.md format)
  - d6aac10: `:package: NEW: implement Phase 2 bytecode optimizations`
  - b3e9c60: `:ok_hand: IMPROVE: add comprehensive VM optimization tests`
  - 5d150dd: `:book: DOC: document Phase 2 VM optimizations completion`

---

## Gotchas (Read This Next Time)

- **Gotcha:** Module not found error when adding new module
  - **Symptom:** `error[E0432]: unresolved import 'crate::optimizer'` in compiler.rs
  - **Root cause:** New modules must be declared in BOTH `main.rs` AND `lib.rs` in Ruff's dual-module structure
  - **Fix:** Added `mod optimizer;` to both `src/main.rs` (line 9) and `src/lib.rs` (line 9)
  - **Prevention:** When adding new `.rs` files, always update both module declaration files. This is a Rust project structure requirement, not Ruff-specific, but easy to forget.

- **Gotcha:** Division by zero must NOT be constant-folded
  - **Symptom:** Test expected div-by-zero to be caught at runtime, but would fail at compile time if folded
  - **Root cause:** Optimizer eagerly evaluates all constant operations; `10 / 0` would panic during compilation
  - **Fix:** Added explicit guards in `try_fold_binary_op()` for division and modulo: `if *b != 0` and `if *b != 0.0`
  - **Prevention:** Any operation that can fail at runtime (div, mod, array access) must include safety checks in the optimizer. Never fold operations that could cause compile-time panics.

- **Gotcha:** Dead code elimination must preserve exception handler metadata
  - **Symptom:** Exception handlers could point to wrong instructions after DCE removed code
  - **Root cause:** BeginTry/BeginCatch/catch_start indices become stale when instructions are removed
  - **Fix:** Built `index_map` during DCE to track old→new instruction indices, then updated all exception handler indices using the map
  - **Prevention:** Any optimization that changes instruction indices must update ALL metadata structures: exception handlers, source maps, debug info. Check `BytecodeChunk` struct for all index-based fields.

- **Gotcha:** Peephole optimization can invalidate later patterns
  - **Symptom:** Some peephole patterns weren't triggering when they should
  - **Root cause:** If you optimize a pattern, you might create a NEW pattern that should also be optimized
  - **Fix:** Iterate through instructions linearly (one pass), accepting that we might miss some opportunities
  - **Prevention:** For more aggressive optimization, would need multiple passes or fixed-point iteration. Current single-pass approach is intentionally simple. If adding more peephole patterns, consider whether they can create each other.

- **Gotcha:** StoreVar + LoadVar pattern is tricky
  - **Symptom:** Initially tried to replace StoreVar+LoadVar with just StoreVar, but that removes the value from stack
  - **Root cause:** StoreVar *consumes* the value. To keep it on stack for subsequent Load, need Dup before Store
  - **Fix:** Replace with Dup + StoreVar (leaves original value on stack)
  - **Prevention:** Understand stack effect of each opcode before optimizing. StoreVar/StoreGlobal are consuming operations. For read-after-write optimization, must Dup first.

- **Gotcha:** Test file variable scoping issues
  - **Symptom:** Loop in `test_vm_optimizations.ruff` showed unexpected results when reusing common variable names like `sum` and `i`
  - **Root cause:** Ruff's variable scoping is global by default within a script file; variable reassignment doesn't create new bindings
  - **Fix:** Used unique variable names (`loop_sum`, `loop_i`) to avoid conflicts with earlier test variables
  - **Prevention:** In test files, use uniquely-named variables for each test, or wrap tests in functions to create isolated scopes. This is a Ruff language characteristic, not a bug.

---

## Things I Learned

### Optimizer Architecture Pattern

- **Three-pass design works well**: constant folding → dead code elimination → peephole
  - Order matters: constant folding creates more dead code, DCE makes peephole simpler
  - Each pass is independent and can be tested separately
  - Recursive application to nested functions (via constant pool) is elegant

- **Optimization is transparent**: All 198 existing tests pass without modification
  - Good sign that optimizations preserve semantics
  - Debug logging (`cfg!(debug_assertions)`) gives visibility without affecting release builds

- **Statistics tracking is valuable**: `OptimizationStats` struct helps validate effectiveness
  - "19 constants folded, 44 dead instructions removed" provides concrete metrics
  - Could be extended to track optimization time, bytecode size reduction percentage

### Constant Folding Rules

- **Safe operations only**: Only fold operations that are guaranteed to succeed
  - Division by zero: skip
  - Floating point operations: safe (NaN/Infinity are valid results)
  - Integer overflow: Rust debug builds would panic, but folding uses checked arithmetic
  
- **Type promotion matters**: Mixed int/float operations need explicit promotion
  - `Constant::Int(a) + Constant::Float(b)` → `Constant::Float(*a as f64 + b)`
  - Both orderings must be handled (int+float and float+int)

- **Deduplication is built-in**: `BytecodeChunk::add_constant()` already deduplicates
  - Don't need to worry about creating duplicate constants during folding
  - Constant pool stays compact automatically

### Dead Code Elimination Algorithm

- **Reachability analysis is graph traversal**: Mark reachable starting from entry points
  - Entry point 0 (main execution)
  - All exception handler catch blocks (can be jumped to on throw)
  - Recursive marking follows all control flow paths

- **Jump target fixup is critical**: After removing instructions, ALL jumps must be updated
  - Built HashMap<old_index, new_index> during elimination
  - Updated Jump, JumpIfFalse, JumpIfTrue, JumpBack, BeginTry opcodes
  - Also updated exception_handlers struct (try_start, try_end, catch_start)

- **Control flow terminators stop propagation**: Return, ReturnNone, Throw end reachability
  - Code after these is dead unless there's a label/jump target leading to it
  - Currently we don't have labeled jumps, so code after terminator is always dead

### Peephole Optimization Insights

- **Pattern matching is simple but effective**: Look for 2-3 instruction sequences
  - LoadConst + Pop → eliminate both (useless load)
  - Jump(target1) where target1 is Jump(target2) → Jump(target2) (skip intermediate)
  - StoreVar(x) + LoadVar(x) → Dup + StoreVar(x) (avoid reload)

- **More patterns exist but have diminishing returns**:
  - Could add: LoadVar(x) + LoadVar(x) → LoadVar(x) + Dup
  - Could add: Pop + Pop → custom DoubleP op instruction (not worth complexity)
  - Keep it simple: focus on high-impact patterns only

### Integration Points

- **Compiler imports optimizer**: Only `Optimizer` struct needs to be public
  - `OptimizationStats` can be public for introspection
  - Optimization passes are private implementation details

- **Optimization is opt-in at compile API level**: `compile_with_optimization(statements, bool)`
  - Default `compile()` always optimizes (calls with `true`)
  - Could add CLI flag `--no-optimize` for debugging if needed

---

## Debug Notes

### Initial compilation error

- **Failing build:** `error[E0432]: unresolved import 'crate::optimizer'`
- **Repro steps:** 
  1. Created `src/optimizer.rs`
  2. Added `use crate::optimizer::Optimizer;` to `src/compiler.rs`
  3. Added `pub mod optimizer;` to `src/lib.rs`
  4. Ran `cargo build`
- **Breakpoints / logs used:** Read compiler error message pointing to compiler.rs:8
- **Final diagnosis:** Forgot to add `mod optimizer;` to `src/main.rs`. Ruff uses both a library crate (lib.rs) and a binary crate (main.rs), so modules must be declared in both for full visibility.

### Test file loop behavior

- **Failing test:** Test 15 in `test_vm_optimizations.ruff` printed "1" instead of expected "30"
- **Repro steps:**
  1. Ran full test file
  2. Loop sum variable showed incorrect value
  3. Isolated loop test into separate file (`/tmp/test_loop.ruff`)
  4. Isolated test worked correctly
- **Breakpoints / logs used:** Added print statements in loop body
- **Final diagnosis:** Variable name conflicts in flat script scope. Using `sum` in multiple places caused later assignments to override earlier values. Not a bug - Ruff's variable scoping is global within script files. Fixed by using unique variable names (`loop_sum`, `loop_i`).

### Division by zero handling

- **Observation:** Test expected runtime error for `10 / 0`, but we fold constants at compile time
- **Design decision:** Added guards to prevent folding division/modulo when divisor is zero
- **Code location:** `optimizer.rs::try_fold_binary_op()` lines 130-135 (int) and 145-149 (float)
- **Rationale:** Division by zero should be a runtime error, not a compile-time panic. Folding it would break error handling semantics.

---

## Follow-ups / TODO (For Future Agents)

- [ ] **Phase 3: JIT Compilation** (Next priority - see ROADMAP.md)
  - Cranelift backend integration
  - Hot path detection
  - Native code generation
  - Expected 5-10x additional speedup

- [ ] **Optional: More aggressive optimizations** (Low priority)
  - Common subexpression elimination (CSE) - detect repeated calculations
  - Inline caching for polymorphic operations - cache type checks
  - Loop-invariant code motion - hoist constant calculations out of loops
  - Note: These are diminishing returns. Focus on JIT for big wins.

- [ ] **Optional: Optimization CLI flag** (Low priority)
  - Add `--no-optimize` flag to disable optimizations for debugging
  - Useful for comparing optimized vs unoptimized bytecode
  - Could also add `--dump-bytecode` to inspect generated code

- [ ] **Optional: Benchmark suite improvements** (Low priority)
  - Expand `examples/benchmark_simple.ruff` to test optimization impact
  - Measure constant folding effectiveness on real code
  - Compare VM with/without optimizations

---

## Links / References

### Files Touched
- `src/optimizer.rs` (NEW - 650 lines)
- `src/compiler.rs` (modified - added optimization integration)
- `src/lib.rs` (modified - added module declaration)
- `src/main.rs` (modified - added module declaration)
- `tests/test_vm_optimizations.ruff` (NEW - 155 lines)
- `CHANGELOG.md` (updated - Phase 2 documentation)
- `ROADMAP.md` (updated - marked Phase 2 complete)
- `README.md` (updated - status and results)

### Related Documentation
- `.github/AGENT_INSTRUCTIONS.md` - Followed git workflow and commit message standards
- `ROADMAP.md` - Phase 2 requirements and Phase 3 next steps
- `src/bytecode.rs` - OpCode definitions and BytecodeChunk structure
- `src/vm.rs` - VM execution model (helps understand optimization impact)

### Key Structures
- `Optimizer` struct - Main optimization coordinator
- `OptimizationStats` - Tracks optimization effectiveness
- `BytecodeChunk` - Contains instructions, constants, exception handlers
- `Constant` enum - Values that can be folded at compile time
- `OpCode` enum - All VM instructions that can be optimized

### External Resources
- Rust compiler optimizations (inspiration for pattern matching approach)
- V8 JavaScript engine's Crankshaft optimizer (dead code elimination technique)
- LuaJIT peephole optimizer (simple pattern matching approach)
